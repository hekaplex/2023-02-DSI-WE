{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl  \n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%pip install scikit-learn  --upgrade --force\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Reading\n",
    "#df= pd.read_csv(\"MasterFile_6-1-23.csv\", index_col=0)\n",
    "#df= pd.read_csv(\"MasterFile_6-1-23.csv\")\n",
    "dfML = pd.read_csv(\"MasterFile_6-18-23.csv\" )\n",
    "dfML_S = pd.read_csv(\"MasterFile_6-18-23.csv\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# !pip install openpyxl  \n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight') \n",
    "import seaborn as sns \n",
    "sns.set_style('darkgrid')\n",
    "from matplotlib import style\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "%pip install scikit-learn  --upgrade --force\n",
    "%pip install xgboost\n",
    "%pip install lightgbm\n",
    "%pip install yellowbrick\n",
    "%pip install Lasso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ML \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn import linear_model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import (TimeSeriesSplit, KFold, StratifiedKFold, GroupKFold, StratifiedGroupKFold)\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['DB'])\n",
    "sns.distplot(dfML['OB'])\n",
    "sns.distplot(dfML['Stroke'])\n",
    "\n",
    "plt.xlabel('Diabetes vs. Obesity')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.DB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.OB.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.Stroke.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['ASM'])\n",
    "sns.distplot(dfML['COPD'])\n",
    "sns.distplot(dfML['Stroke'])\n",
    "\n",
    "plt.xlabel('Asthma vs. Smoking')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.ASM.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.SMK.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['HC'])\n",
    "sns.distplot(dfML['HBP'])\n",
    "sns.distplot(dfML['HD'])\n",
    "sns.distplot(dfML['Stroke'])\n",
    "\n",
    "plt.xlabel('High Chol. vs. High Bld. Pressure VS. Heart Dis. Vs. Stroke')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.HC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.HD.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.HBP.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['ASM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.ASM.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(dfML['HBP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['HBP'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['ASM'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.ASM.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "plt.hist(dfML.ASM, bins=20, rwidth=0.8, density=True)\n",
    "plt.xlabel('Asthma')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "rng = np.arange(dfML.ASM.min(), dfML.ASM.max(), 0.1)\n",
    "plt.plot(rng, norm.pdf(rng,dfML.ASM.mean(), dfML.ASM.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "plt.hist(dfML.ASM, bins=50, rwidth=0.8, density=True)\n",
    "plt.xlabel('Asthma')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "rng = np.arange(dfML.ASM.min(), dfML.ASM.max(), 0.1)\n",
    "plt.plot(rng, norm.pdf(rng,dfML.ASM.mean(), dfML.ASM.std()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['Stroke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "plt.hist(dfML.Stroke, bins=50, rwidth=0.8, density=True)\n",
    "plt.xlabel('Stroke')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "rng = np.arange(dfML.Stroke.min(), dfML.Stroke.max(), 0.1)\n",
    "plt.plot(rng, norm.pdf(rng,dfML.Stroke.mean(), dfML.Stroke.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['DB'])\n",
    "sns.distplot(dfML['OB'])\n",
    "\n",
    "plt.xlabel('Diabestes vs. Obesity Population')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['HC'])\n",
    "sns.distplot(dfML['HD'])\n",
    "sns.distplot(dfML['HBP'])\n",
    "\n",
    "plt.xlabel('High Cho vs. Heart Dis vs. Blood Pressure')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Risk Factors vs. Target Stroke')\n",
    "plt.tight_layout()\n",
    "df_corr = dfML[['DB', 'OB', 'HD', 'HC', 'HBP', 'SMK', 'COPD', 'ASM', 'Stroke']].corr()\n",
    "sns.heatmap(df_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have categorical value, use the code below.\n",
    "#categorical_features = dfML.select_dtypes(include=[np.object])\n",
    "#categorical_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting numerical value to check the corr. against the target values.\n",
    "numeric_features = dfML.select_dtypes(include=[np.number])\n",
    "numeric_features.columns\n",
    "\n",
    "# checking the corr. against the target value.\n",
    "correlation = numeric_features.corr()\n",
    "print(correlation['Stroke'].sort_values(ascending = False), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing  Scaler with dfML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subplot\n",
    "\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\n",
    "fig, axs = plt.subplots(1, len(dfML.columns), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(dfML.iloc[:,i], flierprops=red_circle) # exclude row#0 and #1\n",
    "    ax.set_title(dfML.columns[i], fontsize=20, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this 'file' dataframe for viz only.\n",
    "dfML_S.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anomaly_inputs = ['DB', 'OB', 'HD', 'HC', 'HBP', 'SMK', 'COPD', 'ASM', 'Stroke']\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "model_viz = IsolationForest(contamination=0.1, random_state=42)\n",
    "model_viz.fit(dfML_S[anomaly_inputs])\n",
    "\n",
    "dfML_S['anomaly_scores'] = model_viz.decision_function(dfML_S[anomaly_inputs])\n",
    "dfML_S['anomaly'] = model_viz.predict(dfML_S[anomaly_inputs])\n",
    "\n",
    "palette = ['#ff7f0e', '#1f77b4']\n",
    "sns.pairplot(dfML_S, vars=anomaly_inputs, hue='anomaly', palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_plot01 = ['DB', 'OB', 'HD', 'HC', 'HBP', 'SMK', 'COPD', 'ASM', 'Stroke']\n",
    "sns.pairplot(dfML_S[col_to_plot01], diag_kind='kde', kind='reg', plot_kws={'line_kws':{'color':'brown'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diab = dfML['DB']\n",
    "Target = dfML['Stroke']  # dependent feature (Stroke).\n",
    "plt.scatter(Diab,Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFIND X y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfML.iloc[:,:-1]\n",
    "y = dfML.iloc[:, -1]  # dependent feature (Stroke).\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X,y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train - viz if any variables may have highest corr. .90 or higher that may consider dup. features.\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = X_train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check corr. every variables on the training set for corr. \n",
    "def correlation(dfML, threshold):\n",
    "    col_corr = set() # set of all the names of corrected columns.\n",
    "    corr_matrix = dfML.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i,j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "\n",
    "# calling the def and passing the aug.\n",
    "corr_features = correlation(X_train, 0.90) # the the threshold for the variable\n",
    "len(set(corr_features))\n",
    "\n",
    "        \n",
    "corr_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the dataframe using MinMaxScaler() on dfML dataframe.\n",
    "Scaling using StandarScaler() on dfMS_S data frame for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying MinMaxScaler to dfML dataframe.\n",
    "for col in dfML:\n",
    "    mx = MinMaxScaler()\n",
    "    dfML[col] = mx.fit_transform(dfML[[col]])\n",
    "    \n",
    "\n",
    "\n",
    "# applying StardardScaler to dfMS_S dataframe.\n",
    "for col in dfML_S:\n",
    "    ss = StandardScaler()\n",
    "    dfML_S[col] = ss.fit_transform(dfML_S[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the corr. against the target value after MinMax applied.\n",
    "correlation_MinMax = numeric_features.corr()\n",
    "print(correlation_MinMax['Stroke'].sort_values(ascending = False), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Error Plot.\n",
    "\n",
    "Plot the actual targets from the dataset against the predicted values.  Use this plot to compare against the 45 degree line, where the prediction exactly matches the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the linear model and visualizer\n",
    "%pip install Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from yellowbrick.regressor import prediction_error\n",
    "from yellowbrick.model_selection import FeatureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the train and test data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "model_Las = Lasso()\n",
    "visualizer = prediction_error(model_Las, X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importances: rank features by relative importance in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import FeatureCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import FeatureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/load into a model\n",
    "modelReg = LinearRegression ()\n",
    "modelReg.fit(X_train,y_train)\n",
    "\n",
    "viz = FeatureImportances(modelReg)\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTT THE X AND Y THAN BEST FIT LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR LOOP FOR ALL MODELS - WITHOUT SCALING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"Linear Regression\": LinearRegression(),\n",
    "        \"Decision Tree \": DecisionTreeRegressor(),\n",
    "        \"XGB Regressor \": XGBRegressor(),\n",
    "        \"Random Forest \": RandomForestRegressor(n_estimators=20, random_state=1)\n",
    "        }\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train,y_train) #Train each model\n",
    "    \n",
    "    #Make predction and training and testing sets.\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "        \n",
    "    #Score and error\n",
    "    r_Sq = model.score(X,y)\n",
    "    r2_Score = (r2_score(y_test,y_test_pred)) # reduction by using using least square to fit the line.\n",
    "    MSE_err = (mean_squared_error(y_test,y_test_pred))\n",
    "    MAE_err = (mean_absolute_error(y_test_pred,y_test))\n",
    "    MAPE_err = (mean_absolute_percentage_error(y_test_pred,y_test)) \n",
    "    \n",
    "    \n",
    "    # Print \n",
    "    print(list(models.keys())[i])\n",
    "    \n",
    "    \n",
    "    #print('Coef. of determ. Score', r_Sq)\n",
    "    print('r2 Score = ', r2_Score)\n",
    "    print('MSE err = ', MSE_err)\n",
    "    print('MAE err = ', MAE_err)\n",
    "    print('MAPE err = ', MAPE_err)\n",
    "        \n",
    "    print('\\n')\n",
    "    \n",
    "print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression ML using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit/load into a model\n",
    "modelReg = LinearRegression ()\n",
    "modelReg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "\n",
    "y_train_pred = modelReg.predict(X_train)\n",
    "y_test_pred = modelReg.predict(X_test)\n",
    "#-========================================\n",
    "\n",
    "print('Linear Regression ML using MinMaxScaler')\n",
    "r2_Score = (r2_score(y_test,y_test_pred))\n",
    "r2_Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESIDUAL ON THE REGRESSION TESTING MOMDEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df for the results\n",
    "# pred_y_test_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_test_pred, 'absErrors':abs(y_test - y_test_pred) })\n",
    "# pred_y_df[0:20]\n",
    "\n",
    "\n",
    "y_pred_test_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_test_pred, 'absErrors':abs(y_test - y_test_pred), 'Percentage_Err':((abs(y_test_pred - y_test)) / y_test) * 100 })\n",
    "y_pred_test_df[0:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_df.absErrors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_test_df['absErrors'], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg_Test_Target_value = y_pred_test_df['Actual Stroke Value']\n",
    "Reg_Test_Predicted = y_pred_test_df['Predicted Value']  # dependent feature (Stroke).\n",
    "plt.scatter(Reg_Test_Target_value,Reg_Test_Predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESIDUAL ON THE REGRESSION TRAINING MOMDEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_df = pd.DataFrame({'Actual Stroke Value':y_train, 'Predicted Value':y_train_pred, 'absErrors':abs(y_train - y_train_pred), 'Percentage_Err':((abs(y_train_pred - y_train)) / y_train) * 100 })\n",
    "y_pred_train_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_df.absErrors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred_train_df['absErrors'], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression ML using MinMaxScaler\n",
    "Applied Ridge with GridSearchCV parmeters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge()\n",
    "parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n",
    "ridge_ModelReg=GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_ModelReg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge_ModelReg.best_params_)\n",
    "print(ridge_ModelReg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "y_pred_train_set_ridge = modelReg.predict(X_train)\n",
    "y_pred_test_set_ridge = modelReg.predict(X_test)\n",
    "#-========================================\n",
    "print('MinMaxScaler - Ridge with GridSearchCV ')\n",
    "print('Score', r2_score(y_test,y_pred_test_set_ridge)) # r2 = SSR / SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression ML using StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs= dfML_S.iloc[:,:-1]\n",
    "ys = dfML_S.iloc[:, -1]  # dependent feature (Stroke).\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs,ys,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit StandardScaler/load into a model\n",
    "modelReg = LinearRegression ()\n",
    "modelReg.fit(Xs_train,ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "y_pred_train_set_ss = modelReg.predict(Xs_train)\n",
    "y_pred_test_set_ss = modelReg.predict(Xs_test)\n",
    "#-========================================\n",
    "print('Linear Regressing using StandardScale')\n",
    "print('r2 Score', r2_score(ys_test,y_pred_test_set_ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = modelReg.predict(X_test) # predict X(all factors).\n",
    "# modelReg.score(X_test,y_test) # See the performance (diff of actual value(y_test) from predicted X_test)\n",
    "# Creating new df for the results\n",
    "# pred_y_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_pred, 'absErrors':abs(y_test-y_pred),  'Sqr':(y_test - y_pred)*(y_test - y_pred),  'Percentage_Err':((abs(y_pred - y_test)) / y_test) * 100 })\n",
    "# pred_y_df[0:20]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler Model - Ridge with GridesearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelReg_Ridge=Ridge()\n",
    "parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\n",
    "ModelReg_Ridge_CV=GridSearchCV(ModelReg_Ridge,parameters,scoring='neg_mean_squared_error', cv=10)\n",
    "ModelReg_Ridge_CV.fit(Xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ModelReg_Ridge_CV.best_params_)\n",
    "print(ModelReg_Ridge_CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_set_RidgeCV = ModelReg_Ridge_CV.predict(Xs_train)\n",
    "y_pred_test_Set_RidgeCV = ModelReg_Ridge_CV.predict(Xs_test)\n",
    "\n",
    "print('StandardScaler - Ridge with GridesearchCV ')\n",
    "print('Score', r2_score(ys_test,y_pred_Test_Set_RidgeCV)) # r2 = SSR / SST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals Plot on MinMAXScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_concrete\n",
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_MinMax= Ridge()\n",
    "visualizer = ResidualsPlot(ridge_MinMax)\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(ridge_MinMax, hist=False)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visualizer = ResidualsPlot(ridge_MinMax, hist=False, qqplot=True)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual on StandardScaler Ridge Model.\n",
    "# #esiduals plot to analyze the variance of the error of the regressor. \n",
    "# the points are dispersed around the horizontal axis,\n",
    "# The model is appropriate for the data.\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = FeatureImportances(model)\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['DB'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap - illustrate from the low to high scales. \n",
    "# Looking for the highest (positive) or (negative) corr. features to the target.\n",
    "# Which data are more predicted than the others. Which features have strong corr. or not to the target.\n",
    "\n",
    "plt.title('Risk Factors vs. Target Stroke')\n",
    "plt.tight_layout()\n",
    "df_corr = dfML[['DB', 'OB', 'HD', 'HC', 'HBP', 'SMK', 'COPD', 'ASM', 'Stroke']].corr()\n",
    "sns.heatmap(df_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the corr against the Target_value.\n",
    "dfML.corr()['Stroke']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes vs. Diabetes Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['DB'])\n",
    "plt.xlabel('DB- Data too peak in the middle - right skew.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orginal values\n",
    "print(dfML['DB'].describe())\n",
    "print('Skewness',  dfML['DB'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['DB'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orginal values\n",
    "print(dfML['Stroke'].describe())\n",
    "print('Skewness',  dfML['Stroke'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Stroke'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_DB'] = np.log((dfML.DB)+.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML.Log_DB.describe())\n",
    "print(('Skewness',  dfML['Log_DB'].skew()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Log_DB'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_DB'].describe())\n",
    "print('Skewness',  dfML['Log_DB'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['OB'])\n",
    "plt.xlabel('OB- Data with fat tails - right skew.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original values\n",
    "print(dfML['OB'].describe())\n",
    "print('Skewness',  dfML['OB'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['OB'], bins = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_OB'] = np.log((dfML.OB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Log_OB'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_OB'].describe())\n",
    "print('Skewness',  dfML['Log_OB'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Stroke'], bins = 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log applied to Target + 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing - orginal values\n",
    "dfML['Stroke'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_Target'] = np.log((dfML.Stroke)+20.00)\n",
    "dfML['Log_Target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Log_Target'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_Target'].describe())\n",
    "print('Skewness',  dfML['Log_Target'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.corr()['Log_Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['DB'])\n",
    "sns.distplot(dfML['OB'])\n",
    "\n",
    "plt.xlabel('Diabestes vs. Obesity Population')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['HC'])\n",
    "plt.xlabel('HC- skinny tails and high peak in the middle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['HC'].describe())\n",
    "print('Skewness',  dfML['HC'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['HC'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_HC'] = np.log((dfML.HC)+15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Log_HC'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_HC'].describe())\n",
    "print('Skewness',  dfML['Log_HC'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['HD'])\n",
    "plt.xlabel('HD- Data too peak in the middle - right skew.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['HD'].describe())\n",
    "print('Skewness',  dfML['HD'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_HD'] = np.log((dfML.HD) + 15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['Log_HD'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_HD'].describe())\n",
    "print('Skewness',  dfML['Log_HD'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['HBP'])\n",
    "plt.xlabel('HBP- Data too peak in the middle - right skew.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['HBP'].describe())\n",
    "print('Skewness',  dfML['HBP'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dfML['HBP'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML['Log_HBP'] = np.log((dfML.HBP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfML['Log_HBP'].describe())\n",
    "print('Skewness',  dfML['Log_HBP'].skew())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacimg values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfML['HD_Values'].fillna(dfML['HD_Values'].mean(),inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Diabetes vs. Diabetes Stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean value of Diabetes', dfML['Log_DB'].mean())\n",
    "print('Std value of Diabetes', dfML['Log_DB'].std())\n",
    "print('Min value of Diabetes', dfML['Log_DB'].min())\n",
    "print('Max value of Diabetes', dfML['Log_DB'].max())\n",
    "print('Skewness of Diabetes', dfML['Log_DB'].skew())\n",
    "print('-vs-')\n",
    "print('Mean value of Obesity', dfML['Log_OB'].mean())\n",
    "print('Std value of Obesity', dfML['Log_OB'].std())\n",
    "print('Min value of Obesity', dfML['Log_OB'].min())\n",
    "print('Max value of Obesity', dfML['Log_OB'].max())\n",
    "print('Skewness of Obesity:', dfML['Log_OB'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['Log_DB'])\n",
    "sns.distplot(dfML['Log_OB'])\n",
    "\n",
    "\n",
    "plt.xlabel('DB. vs. OB - Log applied ')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Cholestrol vs. Heart Disese vs. High Blood Pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(dfML['Log_HC_Values'])\n",
    "sns.distplot(dfML['Log_HD_Values'])\n",
    "sns.distplot(dfML['Log_HBP_Values'])\n",
    "\n",
    "plt.xlabel('High Cho. vs. Heart Dis. vs. High Bld Pressure - Log applied')\n",
    "plt.ylabel('Median Measure(Density)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Cholestrol vs. Heart Disese vs. High Blood Pressure Stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean value of High cholestrol', dfML['Log_HC_Values'].mean())\n",
    "print('Std value of  High cholestrol', dfML['Log_HC_Values'].std())\n",
    "print('Min value of  High cholestrol', dfML['Log_HC_Values'].min())\n",
    "print('Max value of  High cholestrol', dfML['Log_HC_Values'].max())\n",
    "print('Skewness of  High cholestrol',  dfML['Log_HC_Values'].skew())\n",
    "\n",
    "print('-vs-')\n",
    "print('Mean value of Heart Disease', dfML['Log_HD_Values'].mean())\n",
    "print('Std value of Heart Disease', dfML['Log_HD_Values'].std())\n",
    "print('Min value of Heart Disease', dfML['Log_HD_Values'].min())\n",
    "print('Max value of Heart Diseasee', dfML['Log_HD_Values'].max())\n",
    "print('Skewness of Heart Disease',  dfML['Log_HD_Values'].skew())\n",
    "\n",
    "print('-vs-')\n",
    "print('Mean value of High Blood Pressure', dfML['Log_HBP_Values'].mean())\n",
    "print('Std value of High Blood Pressure', dfML['Log_HBP_Values'].std())\n",
    "print('Min value of High Blood Pressure', dfML['Log_HBP_Values'].min())\n",
    "print('Max value of High Blood Pressure', dfML['Log_HBP_Values'].max())\n",
    "print('Skewness of High Blood Pressure',  dfML['Log_HBP_Values'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping for the subplot - boxplots below\n",
    "dfML_Log = dfML.drop(columns=['Year_Obv', 'States', 'DB_Values', 'DB_Low', 'DB_High', 'OB_Values',\n",
    "       'OB_Low', 'OB_High', 'HD_Values', 'HD_Low', 'HD_High', 'HC_Values',\n",
    "       'HC_Low', 'HC_High', 'HBP_Values', 'HBP_Low', 'HBP_High', 'TargetStroke', ])\n",
    "#dfBP_01 = dfML_01.rename(columns={'DB_Values':'DB', 'DB_Low':'DBLo', 'DB_High':'DBHx', 'OB_Values':'OB', 'OB_Low':'OBLo', 'OB_High':'OBHx',  'HC_Values':'HC', 'HC_Low':'HCLo',  'HC_High':'HCHx',  'HD_Values':'HD',   'HD_Low':'HDLo',  'HD_High':'HDHx', 'HBP_Values':'HBP', 'HBP_Low':'HBPLo','HBP_High':'HBPHx','TargetStroke':'Target' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_circle = dict(markerfacecolor='red', marker='o') ## outliers\n",
    "mean_shape = dict(markerfacecolor='green', marker='D', markeredgecolor='green') ## \n",
    "plt.boxplot(x=dfML_Log['Log_DB_Values'], vert=True, flierprops=red_circle, showmeans=True, meanprops=mean_shape, notch=True);\n",
    "plt.xlabel('Diabetes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log 1p transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an isolation Forest Model to visualize outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anomaly_inputs = ['Log_DB_Values',  'Log_OB_Values',  \n",
    "       'Log_HD_Values',   'Log_HC_Values',  \n",
    "       'Log_HBP_Values', 'Log_Target_Values']\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "model_IF = IsolationForest(contamination=0.1, random_state=42)\n",
    "model_IF.fit(dfML_Log[anomaly_inputs])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log['anomaly_scores'] = model_IF.decision_function(dfML_Log[anomaly_inputs])\n",
    "dfML_Log['anomaly'] = model_IF.predict(dfML_Log[anomaly_inputs])\n",
    "\n",
    "palette = ['#ff7f0e', '#1f77b4']\n",
    "sns.pairplot(dfML_Log, vars=anomaly_inputs, hue='anomaly', palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.title('Risk Factors vs. Target Stroke')\n",
    "plt.tight_layout()\n",
    "df_corr = dfML_Log[['Log_DB_Values',  'Log_OB_Values', 'Log_HD_Values',  'Log_HC_Values','Log_HBP_Values', 'Log_Target_Values']].corr()\n",
    "sns.heatmap(df_corr, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the corr against the Target_value.\n",
    "dfML_Log.corr()['Log_Target_Values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log = dfML.drop(columns=['Year_Obv', 'States', 'DB_Values', 'DB_Low', 'DB_High', 'OB_Values',\n",
    "       'OB_Low', 'OB_High', 'HD_Values', 'HD_Low', 'HD_High', 'HC_Values',\n",
    "       'HC_Low', 'HC_High', 'HBP_Values', 'HBP_Low', 'HBP_High', 'TargetStroke', ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cal. each upper and lower quantiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subplot\n",
    "\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\n",
    "fig, axs = plt.subplots(1, len(dfML_Log.columns), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(dfML_Log.iloc[:,i], flierprops=red_circle) # exclude row#0 and #1\n",
    "    ax.set_title(dfML_Log.columns[i], fontsize=20, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log['Log_OB_Values'].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q01 = dfML_Log['Log_DB_Values'] .quantile(.99)\n",
    "q02 = dfML_Log['Log_DB_Values'] .quantile(0.01)\n",
    "#q03 = dfML['DB_Low'] .quantile(0.90)\n",
    "#q04 =  dfML['DB_Low'] .quantile(0.01)\n",
    "#q05 = dfML['DB_High'] .quantile(0.90)\n",
    "#q06 = dfML['DB_High'] .quantile(0.01)\n",
    "\n",
    "q07 =  dfML_Log['Log_OB_Values'] .quantile(0.95)\n",
    "q08 =  dfML_Log['Log_OB_Values'] .quantile(0.01)\n",
    "#q09 =  dfML['OB_Low'] .quantile(0.95)\n",
    "#q10 =  dfML['OB_Low'] .quantile(0.01)\n",
    "#q11 =  dfML['OB_High'] .quantile(0.95)\n",
    "#q12 =  dfML1['OB_High'] .quantile(0.01)\n",
    "\n",
    "\n",
    "q13 =  dfML_Log['Log_HC_Values'] .quantile(0.95)\n",
    "q14 =  dfML_Log['Log_HC_Values'] .quantile(0.2)\n",
    "#q15 =  dfML['HC_Low'] .quantile(0.95)\n",
    "#q16 =  dfML['HC_Low'] .quantile(0.02)\n",
    "##q17 =  dfML['HC_High'] .quantile(0.95)\n",
    "#q18 =  dfML['HC_High'] .quantile(0.02)\n",
    "\n",
    "\n",
    "q19 =  dfML_Log['Log_HD_Values'] .quantile(0.90)\n",
    "q20 =  dfML_Log['Log_HD_Values'] .quantile(0.01)\n",
    "#q21 = dfML['HD_Low'] .quantile(0.90)\n",
    "#q22 =  dfML['HD_Low'] .quantile(0.01)\n",
    "#q23 =  dfML['HD_High'] .quantile(0.90)\n",
    "#q24 =  dfML['HD_High'] .quantile(0.01)\n",
    "\n",
    "q25 =  dfML_Log['Log_HBP_Values'] .quantile(0.90)\n",
    "q26 = dfML_Log['Log_HBP_Values'] .quantile(0.02)\n",
    "#q27 =  dfML['HBP_Low'] .quantile(0.89)\n",
    "#q28 =  dfML['HBP_Low'] .quantile(0.02)\n",
    "#q29 =  dfML['HBP_High'] .quantile(0.89)\n",
    "#q30 = dfML['HBP_High'] .quantile(0.02)\n",
    "\n",
    "\n",
    "q31 = dfML_Log['Log_Target_Values'].quantile(0.77)\n",
    "q32 = dfML_Log['Log_Target_Values'].quantile(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q31)\n",
    "print(q32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_circle = dict(markerfacecolor='red', marker='o') ## outliers\n",
    "mean_shape = dict(markerfacecolor='green', marker='D', markeredgecolor='green') ## \n",
    "plt.boxplot(x=dfML_Log['Log_DB_Values'], vert=True, flierprops=red_circle, showmeans=True, meanprops=mean_shape, notch=True);\n",
    "plt.xlabel('Diabetes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extacting and Trimming the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfML_Log =(dfML_Log['Log_DB_Values'] >= 1.02) & (dfML_Log['Log_DB_Values'] <= 3.5)\n",
    "#& (dfML_Log['Log_OB_Values'] >= q08) & (dfML_Log['Log_OB_Values'] <= q07) \\\n",
    "#& (dfML_Log['Log_HC_Values'] >= q14) & (dfML_Log['Log_HC_Values'] <= q13) \\\n",
    "#& (dfML_Log['Log_HD_Values'] >= q20) & (dfML_Log['Log_HD_Values'] <= q19) \\\n",
    "#& (dfML_Log['Log_HBP_Values'] >= q26) & (dfML_Log['Log_HBP_Values'] <= q25) \\\n",
    "#& (dfML_Log['TargetStroke'] >= q32) & (dfML_Log['TargetStroke'] <=q31) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_circle = dict(markerfacecolor='red', marker='o') ## outliers\n",
    "mean_shape = dict(markerfacecolor='green', marker='D', markeredgecolor='green') ## \n",
    "plt.boxplot(x=dfML_Log['Log_DB_Values'], vert=True, flierprops=red_circle, showmeans=True, meanprops=mean_shape, notch=True);\n",
    "plt.xlabel('Diabetes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping for pairplot visualization below only - not for other parameters.\n",
    "#df = dfML_Log.drop(columns=['anomaly_scores','anomaly'])\n",
    "#dfML = df_Dp.rename(columns={'DB_Values':'DB', 'DB_Low':'DBLo', 'DB_High':'DBHx', 'OB_Values':'OB', 'OB_Low':'OBLo', 'OB_High':'OBHx',  'HC_Values':'HC', 'HC_Low':'HCLo',  'HC_High':'HCHx',  'HD_Values':'HD',   'HD_Low':'HDLo',  'HD_High':'HDHx', 'HBP_Values':'HBP', 'HBP_Low':'HBPLo','HBP_High':'HBPHx','TargetStroke':'Target' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subplot\n",
    "\n",
    "red_circle = dict(markerfacecolor='red', marker='o', markeredgecolor='white')\n",
    "fig, axs = plt.subplots(1, len(dfML_Log.columns), figsize=(20,10))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.boxplot(dfML_Log.iloc[:,i], flierprops=red_circle) # exclude row#0 and #1\n",
    "    ax.set_title(dfML_Log.columns[i], fontsize=20, fontweight='bold')\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dfML dataframe without outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_plot01 = ['Log_DB_Values', 'Log_OB_Values', 'Log_HC_Values','Log_HD_Values', 'Log_HBP_Values', 'Log_Target_Values',]\n",
    "sns.pairplot(dfML_Log[col_to_plot01], diag_kind='kde', kind='reg', plot_kws={'line_kws':{'color':'brown'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Log.corr()['Log_Target_Values']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df - ML model including \"statenames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Final = dfML_Log.drop(columns=['Log_Target_Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfML_Final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dummies = pd.get_dummies(df02.States)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only merging the stateName (Dummies)\n",
    "#df_merged = pd.concat([df02, df_dummies],axis='columns')\n",
    "#df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping - including the target -> 'TargetStroke'\n",
    "#df_Final = df_merged.drop(['TargetStroke', 'States'],axis='columns' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML - Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test train split for supervised training.\n",
    "Using the testing portion to predict how well the model perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= dfML_Final\n",
    "y = dfML_Log['Log_Target_Values']  # dependent feature (Stroke).\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit/load into a model\n",
    "modelReg = LinearRegression ()\n",
    "modelReg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================\n",
    "score_train = modelReg.predict(X_train)\n",
    "score_test = modelReg.predict(X_test)\n",
    "#-========================================\n",
    "\n",
    "modelReg.score(X_test,y_test) # See the performance (diff of actual value(y_test) from predicted X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df for the results. What is the probility of someone have the stroke based on the results.\n",
    "pred_y_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_pred, 'absErrors':abs(y_test-y_pred),  'Sqr':(y_test-y_pred)*(y_test-y_pred),  'Percentage_Err':((abs(y_pred - y_test)) / y_test) * 100 })\n",
    "pred_y_df[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(modelReg.score(X_test,y_test))\n",
    "#print(r2_score(y_test,y_pred)) # the accuracy of the prediction and the different is the errors(SSE).\n",
    "print('Score', r2_score(y_test,y_pred)) # r2 = SSR / SST\n",
    "print(\"r2:{}  \".format(r2_score(y_pred,y_test)))  \n",
    "\n",
    "print(\"mae:{}  \".format(mean_absolute_error(y_pred,y_test)))\n",
    "print('MAE = ', mean_absolute_error(y_pred,y_test))\n",
    "#print('MAPE = ', mean_absolute_percentage_error(y_pred,y_test))  # Verify error\n",
    "\n",
    "#==============================================================\n",
    "Tot_Err = pred_y_df['absErrors'].count()\n",
    "#==============================================================\n",
    "\n",
    "\n",
    "# Cal. MAD\n",
    "Sum_absErr = pred_y_df['absErrors'].sum()\n",
    "print('MAD = ', Sum_absErr / Tot_Err)\n",
    "\n",
    "\n",
    "# Cal. MSE\n",
    "Tot_Sum_Sqr = Sum_Sqr = pred_y_df['Sqr'].sum()\n",
    "print('MSE = ', Tot_Sum_Sqr / Tot_Err)\n",
    "\n",
    "# Cal. MAPE\n",
    "Tot_Percentage_Err = pred_y_df['Percentage_Err'].sum()\n",
    "print('MAPE = ', Tot_Percentage_Err / Tot_Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(y_test-y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "modelXGB= XGBRegressor()\n",
    "modelXGB.fit(X_train,y_train)\n",
    "modelXGB.predict(X_test)\n",
    "#y_test  \n",
    "\n",
    "\n",
    "y_pred = modelXGB.predict(X_test) # Predicting X(all factors).\n",
    "modelXGB.score(X_test,y_test) # The performance.\n",
    "\n",
    "\n",
    "print('Score', r2_score(y_test,y_pred))\n",
    "print(\"r2:{}  \".format(r2_score(y_pred,y_test)))  \n",
    "print(\"mae:{}  \".format(mean_absolute_error(y_pred,y_test)))\n",
    "\n",
    "print('mean_absolute_error = ', mean_absolute_error(y_pred,y_test))\n",
    "print('mean_absolute_percentage_error = ', mean_absolute_percentage_error(y_pred,y_test))  # Verify error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-=================\n",
    "# Creating new df - holding values \n",
    "\n",
    "pred_y_XGB_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_pred, 'absErrors':abs(y_test-y_pred),  'Sqr':(y_test-y_pred)*(y_test-y_pred),  'Percentage_Err':((abs(y_pred - y_test)) / y_test) * 100 })\n",
    "pred_y_XGB_df [0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#==============================================================\n",
    "Tot_Err = pred_y_XGB_df['absErrors'].count()\n",
    "#==============================================================\n",
    "\n",
    "\n",
    "# Cal. MAD\n",
    "Sum_absErr = pred_y_XGB_df['absErrors'].sum()\n",
    "print('MAD = ', Sum_absErr / Tot_Err)\n",
    "\n",
    "\n",
    "# Cal. MSE\n",
    "Tot_Sum_Sqr = Sum_Sqr = pred_y_XGB_df['Sqr'].sum()\n",
    "print('MSE = ', Tot_Sum_Sqr / Tot_Err)\n",
    "\n",
    "# Cal. MAPE\n",
    "Tot_Percentage_Err = pred_y_XGB_df['Percentage_Err'].sum()\n",
    "print('MAPE = ', Tot_Percentage_Err / Tot_Err)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE = ', mean_squared_error(pred_y_XGB_df['Actual Stroke Value'], pred_y_XGB_df['Predicted Value'] ) )\n",
    "\n",
    "#SSE - sum of sqr, the goal is to reduced SSE (compare to SSE before removing the outliers)\n",
    "print(\"SSE = \", pred_y_XGB_df['Sqr'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Analyzing using - Decision Tree Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelDT= DecisionTreeRegressor()\n",
    "modelDT.fit(X_train,y_train)\n",
    "modelDT.predict(X_test)\n",
    "#y_test  \n",
    "\n",
    "\n",
    "y_pred = modelDT.predict(X_test) # Predicting X(all factors).\n",
    "modelDT.score(X_test,y_test) # The performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df - holding values for Decision Tree\n",
    "\n",
    "pred_y_DT_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_pred, 'absErrors':abs(y_test-y_pred),  'Sqr':(y_test-y_pred)*(y_test-y_pred),  'Percentage_Err':((abs(y_pred - y_test)) / y_test) * 100 })\n",
    "pred_y_DT_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score', r2_score(y_test,y_pred))\n",
    "print(\"r2:{}  \".format(r2_score(y_pred,y_test)))  \n",
    "print(\"mae:{}  \".format(mean_absolute_error(y_pred,y_test)))\n",
    "\n",
    "print('mean_absolute_error = ', mean_absolute_error(y_pred,y_test))\n",
    "print('mean_absolute_percentage_error = ', mean_absolute_percentage_error(y_pred,y_test))  # Verify error\n",
    "\n",
    "\n",
    "\n",
    "#==============================================================\n",
    "Tot_Err = pred_y_DT_df['absErrors'].count()\n",
    "#==============================================================\n",
    "\n",
    "\n",
    "# Cal. MAD\n",
    "Sum_absErr = pred_y_DT_df['absErrors'].sum()\n",
    "print('MAD = ', Sum_absErr / Tot_Err)\n",
    "\n",
    "\n",
    "# Cal. MSE\n",
    "Tot_Sum_Sqr = Sum_Sqr = pred_y_DT_df['Sqr'].sum()\n",
    "print('MSE = ', Tot_Sum_Sqr / Tot_Err)\n",
    "\n",
    "# Cal. MAPE\n",
    "Tot_Percentage_Err = pred_y_DT_df['Percentage_Err'].sum()\n",
    "print('MAPE = ', Tot_Percentage_Err / Tot_Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ranks=pd.Series(modelDT.feature_importances_,index=X_train.columns,name='Importance')\n",
    "ax=model_ranks.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML Analyzing using - RandomForestRegressor Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelRF = RandomForestRegressor(n_estimators=20, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "\n",
    "modelRF.fit(X_train,y_train)\n",
    "y_pred = modelRF.predict(X_test)\n",
    "modelRF.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_RF_df = pd.DataFrame({'Actual Stroke Value':y_test, 'Predicted Value':y_pred, 'absErrors':abs(y_test-y_pred),  'Sqr':(y_test-y_pred)*(y_test-y_pred),  'Percentage_Err':((abs(y_pred - y_test)) / y_test) * 100 })\n",
    "pred_y_RF_df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Score', r2_score(y_test,y_pred))\n",
    "print(\"r2:{}  \".format(r2_score(y_pred,y_test)))  \n",
    "print(\"mae:{}  \".format(mean_absolute_error(y_pred,y_test)))\n",
    "print('mean_absolute_error = ', mean_absolute_error(y_pred,y_test))\n",
    "print('mean_absolute_percentage_error = ', mean_absolute_percentage_error(y_pred,y_test))  # Verify error\n",
    "print('MSE', mean_squared_error(y_test,y_pred))\n",
    "\n",
    "#==============================================================\n",
    "Tot_Err = pred_y_RF_df['absErrors'].count()\n",
    "#==============================================================\n",
    "\n",
    "\n",
    "# Cal. MAD\n",
    "Sum_absErr = pred_y_RF_df['absErrors'].sum()\n",
    "print('MAD = ', Sum_absErr / Tot_Err)\n",
    "\n",
    "\n",
    "# Cal. MSE\n",
    "Tot_Sum_Sqr = Sum_Sqr = pred_y_RF_df['Sqr'].sum()\n",
    "print('MSE = ', Tot_Sum_Sqr / Tot_Err)\n",
    "\n",
    "# Cal. MAPE\n",
    "Tot_Percentage_Err = pred_y_RF_df['Percentage_Err'].sum()\n",
    "print('MAPE = ', Tot_Percentage_Err / Tot_Err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ranks=pd.Series(modelRF.feature_importances_,index=X_train.columns,name='Importance')\n",
    "ax=model_ranks.plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
